{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. Explain One-Hot Encoding**\n",
    "\n",
    "One-Hot Encoding: It is a technique used to convert categorical data\n",
    "into numerical form for machine learning models. In this technique, each\n",
    "categorical variable is transformed into a binary vector of zeros and\n",
    "ones. Each value of the categorical variable is assigned a unique\n",
    "integer, and then the integer is converted into a binary vector where\n",
    "the value of the corresponding integer is set to 1, and all other values\n",
    "are set to 0.\n",
    "\n",
    "**2. Explain Bag of Words**\n",
    "\n",
    "Bag of Words: It is a technique used to represent text data in numerical\n",
    "form for machine learning models. In this technique, each document is\n",
    "represented as a bag (multiset) of its words, disregarding grammar and\n",
    "word order, but keeping track of the frequency of each word.\n",
    "\n",
    "**3. Explain Bag of N-Grams**\n",
    "\n",
    "Bag of N-Grams: It is a technique similar to Bag of Words, but instead\n",
    "of considering individual words, it considers sequences of N words\n",
    "(N-grams) as features. For example, a bigram (N=2) representation of the\n",
    "sentence \"The quick brown fox jumps over the lazy dog\" would be {The\n",
    "quick, quick brown, brown fox, fox jumps, jumps over, over the, the\n",
    "lazy, lazy dog}.\n",
    "\n",
    "**4. Explain TF-IDF**\n",
    "\n",
    "TF-IDF: Term Frequency-Inverse Document Frequency is a technique used to\n",
    "weigh the importance of words in a text corpus. It is calculated by\n",
    "multiplying the frequency of a word in a document by the logarithm of\n",
    "the total number of documents in the corpus divided by the number of\n",
    "documents containing the word.\n",
    "\n",
    "**5. What is OOV problem?**\n",
    "\n",
    "OOV problem: Out of Vocabulary problem occurs when a word is encountered\n",
    "in the test data that was not present in the training data. This problem\n",
    "can be challenging as it is not possible to predict the meaning of the\n",
    "word from the training data.\n",
    "\n",
    "**6. What are word embeddings?**\n",
    "\n",
    "Word Embeddings: It is a technique used to represent words in a\n",
    "continuous vector space. Each word is represented as a high-dimensional\n",
    "vector, where the position of the vector in the space is learned based\n",
    "on the co-occurrence patterns of the word with other words in a large\n",
    "corpus.\n",
    "\n",
    "**7. Explain Continuous bag of words (CBOW)**\n",
    "\n",
    "Continuous Bag of Words (CBOW): It is a type of neural network\n",
    "architecture used to learn word embeddings. In this architecture, the\n",
    "model predicts the current word given its context (surrounding words),\n",
    "i.e., it predicts the center word based on the surrounding words.\n",
    "\n",
    "**8. Explain SkipGram**\n",
    "\n",
    "SkipGram: It is another type of neural network architecture used to\n",
    "learn word embeddings. In this architecture, the model predicts the\n",
    "surrounding words given the current word, i.e., it predicts the\n",
    "surrounding words based on the center word.\n",
    "\n",
    "**9. Explain Glove Embeddings.**\n",
    "\n",
    "Glove Embeddings: Global Vectors for Word Representation is a technique\n",
    "used to learn word embeddings. It combines the advantages of both CBOW\n",
    "and SkipGram by constructing a co-occurrence matrix of words based on\n",
    "their frequency of co-occurrence in a large corpus, and then factorizing\n",
    "the matrix to obtain low-dimensional word embeddings."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
