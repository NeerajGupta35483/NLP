{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. Explain the architecture of BERT**\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a\n",
    "transformer-based neural network architecture that utilizes a\n",
    "pre-training phase to learn contextual language representations. BERT\n",
    "consists of a stack of encoder layers, where each layer has a multi-head\n",
    "self-attention mechanism and a feedforward neural network. BERT has two\n",
    "variants: BERT base, which has 12 encoder layers, and BERT large, which\n",
    "has 24 encoder layers.\n",
    "\n",
    "**2. Explain Masked Language Modeling (MLM)**\n",
    "\n",
    "Masked Language Modeling (MLM) is a pre-training task in BERT where a\n",
    "random subset of input tokens is replaced with a special \\[MASK\\] token,\n",
    "and the objective is to predict the original token. This task helps the\n",
    "model learn bidirectional representations and understand the context of\n",
    "each token.\n",
    "\n",
    "**3. Explain Next Sentence Prediction (NSP)**\n",
    "\n",
    "Next Sentence Prediction (NSP) is another pre-training task in BERT\n",
    "where the model predicts whether two input sentences are consecutive in\n",
    "a document or not. This task helps the model understand the\n",
    "relationships between sentences and improve its ability to generate\n",
    "coherent text.\n",
    "\n",
    "**4. What is Matthews evaluation?**\n",
    "\n",
    "Matthews evaluation is a binary classification evaluation metric that\n",
    "calculates the Matthews Correlation Coefficient (MCC) between the\n",
    "predicted and true binary labels.\n",
    "\n",
    "**5. What is Matthews Correlation Coefficient (MCC)?**\n",
    "\n",
    "Matthews Correlation Coefficient (MCC) is a measure of the quality of\n",
    "binary (two-class) classifications, ranging from -1 to +1. A coefficient\n",
    "of +1 represents a perfect prediction, 0 represents a random prediction,\n",
    "and -1 represents an inverse prediction.\n",
    "\n",
    "**6. Explain Semantic Role Labeling**\n",
    "\n",
    "Semantic Role Labeling (SRL) is a natural language processing task that\n",
    "involves identifying the semantic roles of phrases in a sentence, such\n",
    "as the agent, patient, and instrument. SRL is used in applications such\n",
    "as question answering, text summarization, and machine translation.\n",
    "\n",
    "**7. Why Fine-tuning a BERT model takes less time than pretraining**\n",
    "\n",
    "Fine-tuning a BERT model takes less time than pre-training because the\n",
    "pre-trained model has already learned contextualized representations of\n",
    "words and phrases, which can be fine-tuned for downstream tasks with\n",
    "relatively little additional training.\n",
    "\n",
    "**8. Recognizing Textual Entailment (RTE)**\n",
    "\n",
    "Recognizing Textual Entailment (RTE) is a natural language processing\n",
    "task that involves determining whether a given text T entails a\n",
    "hypothesis H, contradicts H, or is neutral with respect to H. RTE is\n",
    "used in applications such as question answering, text classification,\n",
    "and information retrieval.\n",
    "\n",
    "**9. Explain the decoder stack of GPT models.**\n",
    "\n",
    "The decoder stack of GPT models consists of multiple layers of decoder\n",
    "blocks, each of which has a multi-head self-attention mechanism and a\n",
    "feedforward neural network. The decoder blocks are similar to those in\n",
    "the encoder of a transformer-based architecture, but with a masked\n",
    "self-attention mechanism that allows the model to attend only to the\n",
    "left side of the sequence, ensuring that the model does not cheat by\n",
    "looking ahead when generating text."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
